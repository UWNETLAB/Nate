{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.pipeline import merge_entities\n",
    "import dill as pickle\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_tags = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_tags = ['PERSON', 'NORP', 'FAC', 'ORG', 'GPE', 'LOC', 'PRODUCT', 'EVENT', 'WORK_OF_ART', 'LAW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = nate.import_csv('../data/sl2.csv', text='content', columns_to_keep=[\"title\", \"case\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df = sl.svo(sub_tags, obj_tags, to_df=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBJECTS = {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "# dependency markers for objects\n",
    "OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "# POS tags that will break adjoining items\n",
    "BREAKER_POS = {\"CCONJ\", \"VERB\"}\n",
    "# words that are negations\n",
    "NEGATIONS = {\"no\", \"not\", \"n't\", \"never\", \"none\"}\n",
    "\n",
    "sub_ner_tags = False\n",
    "obj_ner_tags = False\n",
    "sub_ent_types = []\n",
    "obj_ent_types = []\n",
    "\n",
    "\n",
    "# does dependency set contain any coordinating conjunctions?\n",
    "def contains_conj(depSet):\n",
    "    return \"and\" in depSet or \"or\" in depSet or \"nor\" in depSet or \\\n",
    "           \"but\" in depSet or \"yet\" in depSet or \"so\" in depSet or \"for\" in depSet\n",
    "\n",
    "\n",
    "# get subs joined by conjunctions\n",
    "def _get_subs_from_conjunctions(subs):\n",
    "    more_subs = []\n",
    "    for sub in subs:\n",
    "        # rights is a generator\n",
    "        rights = list(sub.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if contains_conj(rightDeps):\n",
    "            if sub_ner_tags:\n",
    "                more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS and tok.ent_type_ in sub_ner_tags])\n",
    "            else:\n",
    "                more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(more_subs) > 0:\n",
    "                more_subs.extend(_get_subs_from_conjunctions(more_subs))\n",
    "    return more_subs\n",
    "\n",
    "\n",
    "# get objects joined by conjunctions\n",
    "def _get_objs_from_conjunctions(objs):\n",
    "    more_objs = []\n",
    "    for obj in objs:\n",
    "        # rights is a generator\n",
    "        rights = list(obj.rights)\n",
    "        rightDeps = {tok.lower_ for tok in rights}\n",
    "        if contains_conj(rightDeps):\n",
    "            if obj_ner_tags:\n",
    "                more_objs.extend([tok for tok in rights if (tok.dep_ in OBJECTS and tok.ent_type_ in obj_ner_tags) or (tok.pos_ == \"NOUN\" and tok.ent_type_ in obj_ner_tags)])\n",
    "            else:            \n",
    "                more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == \"NOUN\"])\n",
    "            if len(more_objs) > 0:\n",
    "                more_objs.extend(_get_objs_from_conjunctions(more_objs))\n",
    "    return more_objs\n",
    "\n",
    "\n",
    "# find sub dependencies\n",
    "def _find_subs(tok):\n",
    "    head = tok.head\n",
    "    while head.pos_ != \"VERB\" and head.pos_ != \"NOUN\" and head.head != head:\n",
    "        head = head.head\n",
    "    if head.pos_ == \"VERB\":\n",
    "        if sub_ner_tags:\n",
    "            subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\" and tok.ent_type_ in sub_ner_tags]\n",
    "        else:\n",
    "            subs = [tok for tok in head.lefts if tok.dep_ == \"SUB\"]\n",
    "        if len(subs) > 0:\n",
    "            verb_negated = _is_negated(head)\n",
    "            subs.extend(_get_subs_from_conjunctions(subs))\n",
    "            return subs, verb_negated\n",
    "        elif head.head != head:\n",
    "            return _find_subs(head)\n",
    "    elif sub_ner_tags and head.ent_type_ in sub_ner_tags:\n",
    "        return [head], _is_negated(tok)\n",
    "    elif not sub_ner_tags and head.pos_ == \"NOUN\":\n",
    "        return [head], _is_negated(tok)\n",
    "    return [], False\n",
    "\n",
    "\n",
    "# is the tok set's left or right negated?\n",
    "def _is_negated(tok):\n",
    "    parts = list(tok.lefts) + list(tok.rights)\n",
    "    for dep in parts:\n",
    "        if dep.lower_ in NEGATIONS:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# get all the verbs on tokens with negation marker\n",
    "def _find_svs(tokens):\n",
    "    svs = []\n",
    "    verbs = [tok for tok in tokens if tok.pos_ == \"VERB\"]\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = _get_all_subs(v)\n",
    "        if len(subs) > 0:\n",
    "            for sub in subs:\n",
    "                svs.append((sub.orth_, \"!\" + v.orth_ if verbNegated else v.orth_))\n",
    "    return svs\n",
    "\n",
    "\n",
    "# get grammatical objects for a given set of dependencies (including passive sentences)\n",
    "def _get_objs_from_prepositions(deps, is_pas):\n",
    "    objs = []\n",
    "    for dep in deps:\n",
    "        if obj_ner_tags:\n",
    "            if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
    "                objs.extend([tok for tok in dep.rights if (tok.dep_  in OBJECTS and tok.ent_type_ in obj_ner_tags)])\n",
    "                             #(is_pas and tok.ent_type_ in obj_ner_tags and tok.dep_ == 'pobj')]) #temporarily disabled\n",
    "        else:\n",
    "            if dep.pos_ == \"ADP\" and (dep.dep_ == \"prep\" or (is_pas and dep.dep_ == \"agent\")):\n",
    "                objs.extend([tok for tok in dep.rights if tok.dep_ in OBJECTS or\n",
    "                             (tok.pos_ == \"PRON\" and tok.lower_ == \"me\") or\n",
    "                             (is_pas and tok.dep_ == 'pobj')])\n",
    "    return objs\n",
    "\n",
    "\n",
    "# get objects from the dependencies using the attribute dependency\n",
    "# *NOTE* disabled for unknown reason in _get_all_objs, this needs NER option if it should be enabled\n",
    "def _get_objs_from_attrs(deps, is_pas):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"NOUN\" and dep.dep_ == \"attr\":\n",
    "            verbs = [tok for tok in dep.rights if tok.pos_ == \"VERB\"]\n",
    "            if len(verbs) > 0:\n",
    "                for v in verbs:\n",
    "                    rights = list(v.rights)\n",
    "                    objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "                    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "                    if len(objs) > 0:\n",
    "                        return v, objs\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# xcomp; open complement - verb has no suject\n",
    "def _get_obj_from_xcomp(deps, is_pas):\n",
    "    for dep in deps:\n",
    "        if dep.pos_ == \"VERB\" and dep.dep_ == \"xcomp\":\n",
    "            v = dep\n",
    "            rights = list(v.rights)\n",
    "            if obj_ner_tags:\n",
    "                objs = [tok for tok in rights if tok.dep_ in OBJECTS and tok.ent_type_ in obj_ner_tags]\n",
    "            else:\n",
    "                objs = [tok for tok in rights if tok.dep_ in OBJECTS]\n",
    "            objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "            if len(objs) > 0:\n",
    "                return v, objs\n",
    "    return None, None\n",
    "\n",
    "\n",
    "# get all functional subjects adjacent to the verb passed in\n",
    "def _get_all_subs(v):\n",
    "    verb_negated = _is_negated(v)\n",
    "    if sub_ner_tags:\n",
    "        subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.ent_type_ in sub_ner_tags and tok.pos_ != \"DET\"]\n",
    "    else:\n",
    "        subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != \"DET\"]\n",
    "    if len(subs) > 0:\n",
    "        subs.extend(_get_subs_from_conjunctions(subs))\n",
    "    else:\n",
    "        foundSubs, verb_negated = _find_subs(v)\n",
    "        subs.extend(foundSubs)\n",
    "\n",
    "    global sub_ent_types\n",
    "    sub_ent_types = [sub.ent_type_ for sub in subs]\n",
    "        \n",
    "    return subs, verb_negated\n",
    "\n",
    "\n",
    "# is the token a verb?  (excluding auxiliary verbs)\n",
    "def _is_non_aux_verb(tok):\n",
    "    return tok.pos_ == \"VERB\" and (tok.dep_ != \"aux\" and tok.dep_ != \"auxpass\")\n",
    "\n",
    "\n",
    "# return the verb to the right of this verb in a CCONJ relationship if applicable\n",
    "# returns a tuple, first part True|False and second part the modified verb if True\n",
    "def _right_of_verb_is_conj_verb(v):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "\n",
    "    # VERB CCONJ VERB (e.g. he beat and hurt me)\n",
    "    if len(rights) > 1 and rights[0].pos_ == 'CCONJ':\n",
    "        for tok in rights[1:]:\n",
    "            if _is_non_aux_verb(tok):\n",
    "                return True, tok\n",
    "\n",
    "    return False, v\n",
    "\n",
    "\n",
    "# get all objects for an active/passive sentence\n",
    "def _get_all_objs(v, is_pas):\n",
    "    # rights is a generator\n",
    "    rights = list(v.rights)\n",
    "    if obj_ner_tags:\n",
    "        objs = [tok for tok in rights if (tok.dep_ in OBJECTS and tok.ent_type_ in obj_ner_tags) or (is_pas and tok.dep_ == 'pobj' and tok.ent_type_ in obj_ner_tags)]\n",
    "    else:\n",
    "        objs = [tok for tok in rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]\n",
    "    objs.extend(_get_objs_from_prepositions(rights, is_pas))\n",
    "\n",
    "    #potentialNewVerb, potentialNewObjs = _get_objs_from_attrs(rights)\n",
    "    #if potentialNewVerb is not None and potentialNewObjs is not None and len(potentialNewObjs) > 0:\n",
    "    #    objs.extend(potentialNewObjs)\n",
    "    #    v = potentialNewVerb\n",
    "\n",
    "    potential_new_verb, potential_new_objs = _get_obj_from_xcomp(rights, is_pas)\n",
    "    if potential_new_verb is not None and potential_new_objs is not None and len(potential_new_objs) > 0:\n",
    "        objs.extend(potential_new_objs)\n",
    "        v = potential_new_verb\n",
    "    if len(objs) > 0:\n",
    "        objs.extend(_get_objs_from_conjunctions(objs))\n",
    "    \n",
    "    global obj_ent_types\n",
    "    obj_ent_types = [obj.ent_type_ for obj in objs]\n",
    "\n",
    "    return v, objs\n",
    "\n",
    "\n",
    "# return true if the sentence is passive - at he moment a sentence is assumed passive if it has an auxpass verb\n",
    "def _is_passive(tokens):\n",
    "    for tok in tokens:\n",
    "        if tok.dep_ == \"auxpass\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# resolve a 'that' where/if appropriate\n",
    "def _get_that_resolution(toks):\n",
    "    for tok in toks:\n",
    "        if 'that' in [t.orth_ for t in tok.lefts]:\n",
    "            return tok.head\n",
    "    return toks\n",
    "\n",
    "\n",
    "# simple stemmer using lemmas\n",
    "def _get_lemma(word: str):\n",
    "    tokens = nlp(word)\n",
    "    if len(tokens) == 1:\n",
    "        return tokens[0].lemma_\n",
    "    return word\n",
    "\n",
    "\n",
    "# print information for displaying all kinds of things of the parse tree\n",
    "def printDeps(toks):\n",
    "    for tok in toks:\n",
    "        print(tok.orth_, tok.dep_, tok.pos_, tok.head.orth_, [t.orth_ for t in tok.lefts], [t.orth_ for t in tok.rights])\n",
    "\n",
    "\n",
    "# expand an obj / subj np using its chunk\n",
    "def expand(item, tokens, visited):\n",
    "    if item.lower_ == 'that':\n",
    "        item = _get_that_resolution(tokens)\n",
    "\n",
    "    parts = []\n",
    "\n",
    "    if hasattr(item, 'lefts'):\n",
    "        for part in item.lefts:\n",
    "            if part.pos_ in BREAKER_POS:\n",
    "                break\n",
    "            if not part.lower_ in NEGATIONS:\n",
    "                parts.append(part)\n",
    "\n",
    "    parts.append(item)\n",
    "\n",
    "    if hasattr(item, 'rights'):\n",
    "        for part in item.rights:\n",
    "            if part.pos_ in BREAKER_POS:\n",
    "                break\n",
    "            if not part.lower_ in NEGATIONS:\n",
    "                parts.append(part)\n",
    "\n",
    "    if hasattr(parts[-1], 'rights'):\n",
    "        for item2 in parts[-1].rights:\n",
    "            if item2.pos_ == \"DET\" or item2.pos_ == \"NOUN\":\n",
    "                if item2.i not in visited:\n",
    "                    visited.add(item2.i)\n",
    "                    parts.extend(expand(item2, tokens, visited))\n",
    "            break\n",
    "\n",
    "    return parts\n",
    "\n",
    "\n",
    "# convert a list of tokens to a string\n",
    "def to_str(tokens):\n",
    "    return ' '.join([item.text for item in tokens])\n",
    "\n",
    "\n",
    "# find verbs and their subjects / objects to create SVOs, detect passive/active sentences\n",
    "def findSVOs(tokens, sub_tags=False, obj_tags=False):\n",
    "    global sub_ner_tags\n",
    "    sub_ner_tags = sub_tags\n",
    "    global obj_ner_tags\n",
    "    obj_ner_tags = obj_tags\n",
    "    svos = []\n",
    "    is_pas = _is_passive(tokens)\n",
    "    verbs = [tok for tok in tokens if _is_non_aux_verb(tok)]\n",
    "    visited = set()  # recursion detection\n",
    "    sub_ent_types = []\n",
    "    obj_ent_types = []\n",
    "    for v in verbs:\n",
    "        subs, verbNegated = _get_all_subs(v)\n",
    "        # hopefully there are subs, if not, don't examine this verb any longer\n",
    "        if len(subs) > 0:\n",
    "            isConjVerb, conjV = _right_of_verb_is_conj_verb(v)\n",
    "            if isConjVerb:\n",
    "                v2, objs = _get_all_objs(conjV, is_pas)\n",
    "                for sub in subs:\n",
    "                    for obj in objs:\n",
    "                        objNegated = _is_negated(obj)\n",
    "                        if is_pas:  # reverse object / subject for passive\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            sub_ent_types.append(sub.ent_type_)\n",
    "                            obj_ent_types.append(obj.ent_type_)\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v2.lemma_ if verbNegated or objNegated else v2.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            sub_ent_types.append(sub.ent_type_)\n",
    "                            obj_ent_types.append(obj.ent_type_)\n",
    "                        else:\n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                            sub_ent_types.append(sub.ent_type_)\n",
    "                            obj_ent_types.append(obj.ent_type_)                            \n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v2.lower_ if verbNegated or objNegated else v2.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                            sub_ent_types.append(sub.ent_type_)\n",
    "                            obj_ent_types.append(obj.ent_type_)         \n",
    "            else:\n",
    "                v, objs = _get_all_objs(v, is_pas)\n",
    "                for sub in subs:\n",
    "                    for obj in objs:\n",
    "                        objNegated = _is_negated(obj)\n",
    "                        if is_pas:  # reverse object / subject for passive\n",
    "                            svos.append((to_str(expand(obj, tokens, visited)),\n",
    "                                         \"!\" + v.lemma_ if verbNegated or objNegated else v.lemma_, to_str(expand(sub, tokens, visited))))\n",
    "                            sub_ent_types.append(sub.ent_type_)\n",
    "                            obj_ent_types.append(obj.ent_type_)                        \n",
    "                        else:\n",
    "                            svos.append((to_str(expand(sub, tokens, visited)),\n",
    "                                         \"!\" + v.lower_ if verbNegated or objNegated else v.lower_, to_str(expand(obj, tokens, visited))))\n",
    "                            sub_ent_types.append(sub.ent_type_)\n",
    "                            obj_ent_types.append(obj.ent_type_)\n",
    "                            \n",
    "    return (svos, sub_ent_types, obj_ent_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load, Parallel, delayed, cpu_count\n",
    "from toolz import partition_all\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "results = list(itertools.chain(*temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from spacy.util import minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_process(texts, nlp):\n",
    "    processed_list = [doc for doc in nlp.pipe(texts)]\n",
    "    return processed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp(items, function, cpu, *args):\n",
    "    batch_size = round(len(items)/cpu)\n",
    "    partitions = partition_all(batch_size, items)\n",
    "    temp = Parallel(n_jobs=cpu, max_nbytes=None)(delayed(function)(v, *args) for v in partitions)\n",
    "    if isinstance(temp[0], dict):\n",
    "        results = {}\n",
    "        for batch in temp:\n",
    "            for key, value in batch.items():\n",
    "                results.setdefault(key, []).extend(value)\n",
    "    elif isinstance(temp[0], (list, tuple)):\n",
    "        results = list(itertools.chain(*temp))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mp(items, function, cpu, *args):\n",
    "    batch_size = round(len(items)/cpu)\n",
    "    partitions = minibatch(items, size=batch_size)\n",
    "    executor = Parallel(n_jobs=cpu, backend='multiprocessing', prefer='processes')\n",
    "    do = delayed(partial(function, *args))\n",
    "    tasks = (do(batch) for batch in partitions)\n",
    "    temp = executor(tasks)\n",
    "    if isinstance(temp[0], dict):\n",
    "        results = {}\n",
    "        for batch in temp:\n",
    "            for key, value in batch.items():\n",
    "                results.setdefault(key, []).extend(value)\n",
    "    elif isinstance(temp[0], (list, tuple)):\n",
    "        results = list(itertools.chain(*temp))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cpu_count() >= 8:   #to avoid overtaxing Brad, save some cores\n",
    "    cpu = 10\n",
    "else:\n",
    "    cpu = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_svo(text_list, sub_tags = False, obj_tags = False):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.add_pipe(merge_entities)\n",
    "    post_nlp = mp(text_list, spacy_process, cpu, nlp)\n",
    "    sentences = [[x.string.strip() for x in y.sents] for y in post_nlp]\n",
    "    svo_items = [[findSVOs(x, sub_tags, obj_tags) for x in y.sents] for y in post_nlp]\n",
    "\n",
    "    \n",
    "    return sentences, svo_items\n",
    "    \n",
    "    \n",
    "def svo_to_df(sentences, svo_items):\n",
    "    df = pd.DataFrame()\n",
    "    doc_id = []\n",
    "    sent_id = []\n",
    "    sent_list_flat = []\n",
    "    svo_list_flat = []\n",
    "    sub_list_flat = []\n",
    "    verb_list_flat = []\n",
    "    obj_list_flat = []\n",
    "    sub_ent_types = []\n",
    "    obj_ent_types = []\n",
    "    for i, doc in enumerate(sentences):\n",
    "        for j, sent in enumerate(doc):\n",
    "            for k, svo_item in enumerate(svo_items[i][j][0]):\n",
    "                doc_id.append(i)\n",
    "                sent_id.append(j)\n",
    "                sent_list_flat.append(sent)\n",
    "                svo_list_flat.append(svo_item)\n",
    "                sub_list_flat.append(svo_item[0])\n",
    "                verb_list_flat.append(svo_item[1])\n",
    "                obj_list_flat.append(svo_item[2])\n",
    "                sub_ent_types.append(svo_items[i][j][1][k])\n",
    "                obj_ent_types.append(svo_items[i][j][2][k])\n",
    "\n",
    "            \n",
    "    df['doc_id'], df['sent_id'], df ['sentence'], df['svo'] = doc_id, sent_id, sent_list_flat, svo_list_flat\n",
    "    df['subject'], df['sub_type'], df['verb'], df['object'], df['obj_type'] = sub_list_flat, sub_ent_types, verb_list_flat, obj_list_flat, obj_ent_types\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/sl2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svo(sub_tags=False, obj_tags=False):\n",
    "    \"\"\"\n",
    "    This is a docstring\n",
    "    \"\"\" \n",
    "    text_list = df.content.tolist()\n",
    "    sentences, svo_items = process_svo(text_list, sub_tags, obj_tags)\n",
    "\n",
    "    return svo_to_df(sentences, svo_items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "self.c cannot be converted to a Python object for pickling",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 344, in _sendback_result\n    exception=exception))\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\queues.py\", line 234, in put\n    obj = dumps(obj, reducers=self._reducers)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 243, in dumps\n    dump(obj, buf, reducers=reducers, protocol=protocol)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\backend\\reduction.py\", line 236, in dump\n    _LokyPickler(file, reducers=reducers, protocol=protocol).dump(obj)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\site-packages\\joblib\\externals\\cloudpickle\\cloudpickle.py\", line 267, in dump\n    return Pickler.dump(self, obj)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 437, in dump\n    self.save(obj)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 549, in save\n    self.save_reduce(obj=obj, *rv)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 662, in save_reduce\n    save(state)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 859, in save_dict\n    self._batch_setitems(obj.items())\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 885, in _batch_setitems\n    save(v)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 819, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 846, in _batch_appends\n    save(tmp[0])\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 504, in save\n    f(self, obj) # Call unbound method with explicit self\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 819, in save_list\n    self._batch_appends(obj)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 843, in _batch_appends\n    save(x)\n  File \"C:\\Users\\tcrick\\Anaconda3\\lib\\pickle.py\", line 524, in save\n    rv = reduce(self.proto)\n  File \"stringsource\", line 2, in spacy.tokens.doc.Doc.__reduce_cython__\nTypeError: self.c cannot be converted to a Python object for pickling\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-deb1e4001294>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lprun'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-f process_svo svo(sub_tags, obj_tags)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2313\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2314\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2315\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<C:\\Users\\tcrick\\Anaconda3\\lib\\site-packages\\decorator.py:decorator-gen-127>\u001b[0m in \u001b[0;36mlprun\u001b[1;34m(self, parameter_s)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\line_profiler.py\u001b[0m in \u001b[0;36mlprun\u001b[1;34m(self, parameter_s)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m                 \u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_ns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m                 \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\line_profiler.py\u001b[0m in \u001b[0;36mrunctx\u001b[1;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_by_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[0mexec_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_by_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-6681ff54e118>\u001b[0m in \u001b[0;36msvo\u001b[1;34m(sub_tags, obj_tags)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \"\"\" \n\u001b[0;32m      5\u001b[0m     \u001b[0mtext_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvo_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_svo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msvo_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvo_items\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-9f12b9063f79>\u001b[0m in \u001b[0;36mprocess_svo\u001b[1;34m(text_list, sub_tags, obj_tags)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmerge_entities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mpost_nlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspacy_process\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_nlp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0msvo_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfindSVOs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msub_tags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj_tags\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpost_nlp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a76eabbb411c>\u001b[0m in \u001b[0;36mmp\u001b[1;34m(items, function, cpu, *args)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpartitions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartition_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_nbytes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: self.c cannot be converted to a Python object for pickling"
     ]
    }
   ],
   "source": [
    "%lprun -f process_svo svo(sub_tags, obj_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
